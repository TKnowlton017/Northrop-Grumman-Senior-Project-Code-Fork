{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29ab05d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil.parser import parse\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "67f33601",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\peter\\anaconda3\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:31: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4ea11915",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "# opening the zarr_dataset\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = \"../../../prediction-dataset/\"\n",
        "cfg = load_config_data(\"./visualisation_config.yaml\")\n",
        "dm = LocalDataManager()\n",
        "dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\n",
        "zarr_dataset = ChunkedDataset(dataset_path)\n",
        "zarr_dataset.open()\n",
        "\n",
        "# using EgoDataset interface to extract AV data\n",
        "rast = build_rasterizer(cfg, dm)\n",
        "ego_dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
        "print(ego_dataset)\n",
        "# agent_dataset = AgentDataset(cfg, zarr_dataset, rast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9c971117",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[    0   248]\n",
            " [  248   497]\n",
            " [  497   746]\n",
            " [  746   995]\n",
            " [  995  1244]\n",
            " [ 1244  1493]\n",
            " [ 1493  1742]\n",
            " [ 1742  1991]\n",
            " [ 1991  2240]\n",
            " [ 2240  2489]\n",
            " [ 2489  2738]\n",
            " [ 2738  2987]\n",
            " [ 2987  3236]\n",
            " [ 3236  3485]\n",
            " [ 3485  3734]\n",
            " [ 3734  3983]\n",
            " [ 3983  4232]\n",
            " [ 4232  4481]\n",
            " [ 4481  4730]\n",
            " [ 4730  4979]\n",
            " [ 4979  5228]\n",
            " [ 5228  5477]\n",
            " [ 5477  5725]\n",
            " [ 5725  5973]\n",
            " [ 5973  6221]\n",
            " [ 6221  6469]\n",
            " [ 6469  6717]\n",
            " [ 6717  6965]\n",
            " [ 6965  7213]\n",
            " [ 7213  7461]\n",
            " [ 7461  7709]\n",
            " [ 7709  7957]\n",
            " [ 7957  8205]\n",
            " [ 8205  8453]\n",
            " [ 8453  8701]\n",
            " [ 8701  8949]\n",
            " [ 8949  9197]\n",
            " [ 9197  9445]\n",
            " [ 9445  9693]\n",
            " [ 9693  9941]\n",
            " [ 9941 10189]\n",
            " [10189 10437]\n",
            " [10437 10685]\n",
            " [10685 10933]\n",
            " [10933 11182]\n",
            " [11182 11431]\n",
            " [11431 11680]\n",
            " [11680 11929]\n",
            " [11929 12177]\n",
            " [12177 12425]\n",
            " [12425 12673]\n",
            " [12673 12921]\n",
            " [12921 13169]\n",
            " [13169 13417]\n",
            " [13417 13665]\n",
            " [13665 13913]\n",
            " [13913 14161]\n",
            " [14161 14409]\n",
            " [14409 14657]\n",
            " [14657 14905]\n",
            " [14905 15153]\n",
            " [15153 15401]\n",
            " [15401 15649]\n",
            " [15649 15897]\n",
            " [15897 16145]\n",
            " [16145 16393]\n",
            " [16393 16641]\n",
            " [16641 16889]\n",
            " [16889 17137]\n",
            " [17137 17385]\n",
            " [17385 17633]\n",
            " [17633 17881]\n",
            " [17881 18129]\n",
            " [18129 18377]\n",
            " [18377 18625]\n",
            " [18625 18874]\n",
            " [18874 19123]\n",
            " [19123 19372]\n",
            " [19372 19621]\n",
            " [19621 19870]\n",
            " [19870 20118]\n",
            " [20118 20366]\n",
            " [20366 20614]\n",
            " [20614 20862]\n",
            " [20862 21110]\n",
            " [21110 21358]\n",
            " [21358 21607]\n",
            " [21607 21856]\n",
            " [21856 22105]\n",
            " [22105 22354]\n",
            " [22354 22603]\n",
            " [22603 22852]\n",
            " [22852 23101]\n",
            " [23101 23350]\n",
            " [23350 23598]\n",
            " [23598 23846]\n",
            " [23846 24094]\n",
            " [24094 24342]\n",
            " [24342 24590]\n",
            " [24590 24838]]\n"
          ]
        }
      ],
      "source": [
        "# printing the frame ranges that make up each scene\n",
        "scene_frame_indexes = np.array([[0,0]])\n",
        "for i in range(len(zarr_dataset.scenes)):\n",
        "    scene_frame_indexes = np.append(scene_frame_indexes, [[zarr_dataset.scenes[i][0][0], zarr_dataset.scenes[i][0][1]]], axis = 0)\n",
        "    \n",
        "scene_frame_indexes = np.delete(scene_frame_indexes, 0, 0)\n",
        "print(scene_frame_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "ac1949ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "position_AV = np.array([0,0,0,0,0])\n",
        "output = np.array([0,0])\n",
        "startpos = 1\n",
        "endpos = 1000\n",
        "for i in range(startpos, endpos):\n",
        "    position_AV = np.append(position_AV, [ego_dataset[i][\"centroid\"][0], ego_dataset[i][\"centroid\"][1], ego_dataset[i][\"yaw\"], ego_dataset[i][\"target_velocities\"][0][0], ego_dataset[i][\"target_velocities\"][0][1]], axis = 0)\n",
        "    output = np.append(output, [ego_dataset[i][\"target_positions\"][0][0], ego_dataset[i][\"target_positions\"][0][1]], axis = 0)\n",
        "\n",
        "#print(position_AV)\n",
        "# for i in range(scene_frame_indexes[0][0], scene_frame_indexes[0][1]):\n",
        "position_AV = position_AV.astype(np.float32)\n",
        "output = output.astype(np.float32)\n",
        "position_AV = position_AV.reshape(endpos, 5)\n",
        "output = output.reshape(endpos, 2)\n",
        "position_AV = np.delete(position_AV, 0, 0)\n",
        "output = np.delete(output, 0, 0)\n",
        "inputs = torch.from_numpy(position_AV)\n",
        "outputs = torch.from_numpy(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "21f0edc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = TensorDataset(inputs, outputs)\n",
        "val_percent = 0.2\n",
        "num_rows = len(inputs)\n",
        "val_size = int(num_rows * val_percent)\n",
        "train_size = num_rows - val_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "b7e23965",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 0)\n",
        "val_loader = DataLoader(val_ds, batch_size, num_workers = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "ef6645ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = len(inputs[0])\n",
        "output_size = len(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "758cfa15",
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(5, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25,10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10,2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.layers(xb)                       \n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        inputs, targets = batch \n",
        "        # Generate predictions\n",
        "        out = self(inputs)          \n",
        "        # Calcuate loss\n",
        "        loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')  \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        inputs, targets = batch\n",
        "        # Generate predictions\n",
        "        out = self(inputs)\n",
        "        # Calculate loss\n",
        "        loss = F.l1_loss(input = out, target = targets, size_average = None, reduce = None, reduction = 'mean')    \n",
        "        return {'val_loss': loss.detach()}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        return {'val_loss': epoch_loss.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result, num_epochs):\n",
        "        # Print result every 20th epoch\n",
        "        if (epoch+1) % 20 == 0 or epoch == num_epochs-1:\n",
        "            print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch+1, result['val_loss']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "5570e605",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "b2deb000",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0973, -0.1126,  0.1933,  0.0610, -0.1633],\n",
              "         [-0.3800,  0.3714,  0.1236,  0.1791, -0.4268],\n",
              "         [ 0.3055,  0.2364, -0.3286,  0.3521, -0.0177],\n",
              "         [-0.0830,  0.3017, -0.0955, -0.1195,  0.2159],\n",
              "         [-0.3276,  0.3259,  0.3276, -0.0729,  0.4150],\n",
              "         [ 0.0753, -0.2934, -0.1268,  0.1486, -0.3720],\n",
              "         [ 0.1903, -0.3526,  0.2773, -0.1299, -0.1910],\n",
              "         [ 0.2033, -0.0779,  0.3889,  0.1849, -0.0766],\n",
              "         [ 0.1573, -0.2322, -0.3507, -0.0184,  0.1781],\n",
              "         [-0.4284,  0.1971,  0.3300, -0.3992,  0.4261],\n",
              "         [ 0.2911,  0.4219, -0.2861,  0.4449, -0.1113],\n",
              "         [-0.4394, -0.2722, -0.2330,  0.3428,  0.0412],\n",
              "         [-0.0932,  0.1524, -0.1549, -0.1163,  0.3818],\n",
              "         [ 0.2253,  0.4390,  0.3559,  0.3009,  0.2334],\n",
              "         [-0.4177,  0.4445, -0.2785, -0.2987, -0.3044],\n",
              "         [ 0.2863,  0.4009, -0.1670,  0.0617,  0.0697],\n",
              "         [-0.2876, -0.1502, -0.2582,  0.1240, -0.0291],\n",
              "         [ 0.3616,  0.4368,  0.3696, -0.2618, -0.2710],\n",
              "         [ 0.1153,  0.3383, -0.2955,  0.4132, -0.2524],\n",
              "         [ 0.3505,  0.4202, -0.0567, -0.1661,  0.2686],\n",
              "         [ 0.1596, -0.2416, -0.3059, -0.0138, -0.0949],\n",
              "         [ 0.2341,  0.0843,  0.3447, -0.3331, -0.1127],\n",
              "         [-0.3772, -0.0100,  0.2423,  0.3881, -0.3195],\n",
              "         [ 0.2641, -0.2334,  0.0068, -0.0538,  0.1749],\n",
              "         [-0.3049,  0.1277,  0.3432,  0.4005,  0.2280]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.3675,  0.1864,  0.2804, -0.3345, -0.3585, -0.1440,  0.2764,  0.2209,\n",
              "         -0.1438,  0.0480, -0.2797,  0.2528, -0.0894, -0.3761,  0.2387, -0.1019,\n",
              "         -0.2746,  0.2106, -0.1488, -0.2301,  0.0493, -0.1417,  0.3574,  0.4410,\n",
              "         -0.0596], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0833, -0.0002, -0.1728, -0.0653,  0.1694, -0.1772, -0.0065,  0.0776,\n",
              "           0.0212,  0.1602,  0.0980, -0.0993, -0.0365,  0.0199,  0.0893, -0.1863,\n",
              "          -0.0116, -0.1511, -0.1953,  0.0965, -0.0912,  0.0959, -0.1066, -0.1748,\n",
              "           0.1986],\n",
              "         [-0.1113, -0.1246,  0.0163, -0.1837, -0.0615, -0.0833, -0.0955, -0.1276,\n",
              "           0.0480,  0.1944, -0.0850, -0.0952, -0.1830, -0.1994,  0.1120,  0.1424,\n",
              "          -0.1421, -0.0053, -0.1821, -0.0939,  0.1232, -0.0990,  0.1672,  0.1221,\n",
              "           0.0556],\n",
              "         [-0.0765, -0.0314, -0.0107, -0.0687,  0.0959, -0.0649,  0.0348, -0.0139,\n",
              "          -0.1292, -0.1618, -0.1010, -0.0637, -0.0880, -0.0428, -0.1231,  0.1088,\n",
              "          -0.1755,  0.1866, -0.0511, -0.0539, -0.0742, -0.1824, -0.1567, -0.0391,\n",
              "          -0.1212],\n",
              "         [-0.1140, -0.0717, -0.0759, -0.1789,  0.1093,  0.1606, -0.0451, -0.0482,\n",
              "           0.0756, -0.1984,  0.1333,  0.0926,  0.1551, -0.0042, -0.0907,  0.0193,\n",
              "           0.1399, -0.1232,  0.1256, -0.1784,  0.1431, -0.0175, -0.0524, -0.0203,\n",
              "          -0.1592],\n",
              "         [ 0.1192, -0.1685,  0.0053,  0.0779,  0.1085, -0.1121, -0.1775, -0.1566,\n",
              "          -0.0655,  0.1315,  0.0922,  0.0419,  0.0220,  0.0885, -0.1123, -0.1647,\n",
              "           0.1461,  0.1965, -0.0549,  0.1386,  0.0997,  0.1784, -0.1940, -0.1873,\n",
              "          -0.0351],\n",
              "         [-0.0062,  0.1744, -0.0666,  0.0740,  0.1686, -0.0995,  0.0831,  0.0973,\n",
              "           0.0276,  0.0151,  0.0325, -0.1300, -0.1711,  0.1679, -0.1047,  0.0946,\n",
              "          -0.0939, -0.0138, -0.1804, -0.0249,  0.0881, -0.1379, -0.1559,  0.1406,\n",
              "           0.0798],\n",
              "         [-0.1087, -0.0463, -0.1814, -0.1106,  0.1476, -0.0173,  0.0451, -0.1210,\n",
              "          -0.0186,  0.0826, -0.1137, -0.0708,  0.0942, -0.1890,  0.1143,  0.0211,\n",
              "           0.1107, -0.1684,  0.1142, -0.0670,  0.0543,  0.0400, -0.0447, -0.1379,\n",
              "          -0.0660],\n",
              "         [ 0.0217, -0.1236, -0.0072, -0.0221,  0.1774, -0.1564,  0.0921, -0.0794,\n",
              "           0.1185,  0.1925, -0.0375,  0.1619,  0.1900,  0.1125,  0.0657, -0.0401,\n",
              "           0.1142,  0.1169, -0.1239, -0.1277, -0.1433,  0.1456, -0.0894,  0.0667,\n",
              "           0.1441],\n",
              "         [-0.0402, -0.1112,  0.1560, -0.1532,  0.1731, -0.0070,  0.0372, -0.0850,\n",
              "          -0.1701, -0.1313, -0.1029,  0.1341,  0.1067, -0.0988,  0.1685,  0.0707,\n",
              "          -0.1850,  0.1535,  0.1586,  0.0647,  0.0225,  0.0003,  0.1283, -0.1124,\n",
              "           0.1963],\n",
              "         [-0.1120, -0.0858, -0.0222, -0.1215, -0.0932,  0.0874,  0.0796,  0.0512,\n",
              "          -0.1594, -0.1407,  0.1465,  0.1584, -0.0176, -0.0403, -0.0910, -0.0749,\n",
              "           0.0124, -0.1336,  0.0682,  0.1868, -0.1655,  0.1872,  0.0339,  0.1161,\n",
              "           0.0374]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0460, -0.0308,  0.1076, -0.0695,  0.1378, -0.1792, -0.0250, -0.0837,\n",
              "         -0.0885, -0.1939], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.1495,  0.2299,  0.0471, -0.1492, -0.2164, -0.1383,  0.3121, -0.1089,\n",
              "          -0.0597, -0.0423],\n",
              "         [-0.1858, -0.2897,  0.3110, -0.1297,  0.0298, -0.2027,  0.3102, -0.3075,\n",
              "           0.1580,  0.1140]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.2220,  0.0494], requires_grad=True)]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "1f314be3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result, epochs)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "5aa9860a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'val_loss': 44.94728469848633}\n"
          ]
        }
      ],
      "source": [
        "result = evaluate(model, val_loader)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "e97e6247",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20], val_loss: 40.2089\n",
            "Epoch [40], val_loss: 37.0362\n",
            "Epoch [60], val_loss: 34.1831\n",
            "Epoch [80], val_loss: 31.4413\n",
            "Epoch [100], val_loss: 28.8333\n",
            "Epoch [120], val_loss: 26.3411\n",
            "Epoch [140], val_loss: 23.9533\n",
            "Epoch [160], val_loss: 21.6392\n",
            "Epoch [180], val_loss: 19.3894\n",
            "Epoch [200], val_loss: 17.1939\n",
            "Epoch [220], val_loss: 15.0449\n",
            "Epoch [240], val_loss: 12.9346\n",
            "Epoch [260], val_loss: 10.8569\n",
            "Epoch [280], val_loss: 8.8060\n",
            "Epoch [300], val_loss: 6.7756\n",
            "Epoch [320], val_loss: 5.1195\n",
            "Epoch [340], val_loss: 4.5157\n",
            "Epoch [360], val_loss: 4.2832\n",
            "Epoch [380], val_loss: 4.0553\n",
            "Epoch [400], val_loss: 3.8383\n",
            "Epoch [420], val_loss: 3.6454\n",
            "Epoch [440], val_loss: 3.4752\n",
            "Epoch [460], val_loss: 3.3234\n",
            "Epoch [480], val_loss: 3.1836\n",
            "Epoch [500], val_loss: 3.0546\n",
            "Epoch [520], val_loss: 2.9350\n",
            "Epoch [540], val_loss: 2.8342\n",
            "Epoch [560], val_loss: 2.7409\n",
            "Epoch [580], val_loss: 2.6486\n",
            "Epoch [600], val_loss: 2.5563\n",
            "Epoch [620], val_loss: 2.4645\n",
            "Epoch [640], val_loss: 2.3725\n",
            "Epoch [660], val_loss: 2.2808\n",
            "Epoch [680], val_loss: 2.1879\n",
            "Epoch [700], val_loss: 2.0934\n",
            "Epoch [720], val_loss: 1.9966\n",
            "Epoch [740], val_loss: 1.8984\n",
            "Epoch [760], val_loss: 1.8024\n",
            "Epoch [780], val_loss: 1.7142\n",
            "Epoch [800], val_loss: 1.6323\n",
            "Epoch [820], val_loss: 1.5532\n",
            "Epoch [840], val_loss: 1.4764\n",
            "Epoch [860], val_loss: 1.4022\n",
            "Epoch [880], val_loss: 1.3300\n",
            "Epoch [900], val_loss: 1.2591\n",
            "Epoch [920], val_loss: 1.1895\n",
            "Epoch [940], val_loss: 1.1207\n",
            "Epoch [960], val_loss: 1.0474\n",
            "Epoch [980], val_loss: 0.9663\n",
            "Epoch [1000], val_loss: 0.8781\n",
            "Epoch [1020], val_loss: 0.8299\n",
            "Epoch [1040], val_loss: 0.7852\n",
            "Epoch [1060], val_loss: 0.7489\n",
            "Epoch [1080], val_loss: 0.7183\n",
            "Epoch [1100], val_loss: 0.6914\n",
            "Epoch [1120], val_loss: 0.6675\n",
            "Epoch [1140], val_loss: 0.6452\n",
            "Epoch [1160], val_loss: 0.6259\n",
            "Epoch [1180], val_loss: 0.6082\n",
            "Epoch [1200], val_loss: 0.5918\n",
            "Epoch [1220], val_loss: 0.5778\n",
            "Epoch [1240], val_loss: 0.5653\n",
            "Epoch [1260], val_loss: 0.5541\n",
            "Epoch [1280], val_loss: 0.5441\n",
            "Epoch [1300], val_loss: 0.5353\n",
            "Epoch [1320], val_loss: 0.5280\n",
            "Epoch [1340], val_loss: 0.5215\n",
            "Epoch [1360], val_loss: 0.5164\n",
            "Epoch [1380], val_loss: 0.5117\n",
            "Epoch [1400], val_loss: 0.5073\n",
            "Epoch [1420], val_loss: 0.5032\n",
            "Epoch [1440], val_loss: 0.4994\n",
            "Epoch [1460], val_loss: 0.4959\n",
            "Epoch [1480], val_loss: 0.4927\n",
            "Epoch [1500], val_loss: 0.4898\n",
            "Epoch [1520], val_loss: 0.4871\n",
            "Epoch [1540], val_loss: 0.4847\n",
            "Epoch [1560], val_loss: 0.4825\n",
            "Epoch [1580], val_loss: 0.4806\n",
            "Epoch [1600], val_loss: 0.4788\n",
            "Epoch [1620], val_loss: 0.4773\n",
            "Epoch [1640], val_loss: 0.4758\n",
            "Epoch [1660], val_loss: 0.4746\n",
            "Epoch [1680], val_loss: 0.4736\n",
            "Epoch [1700], val_loss: 0.4726\n",
            "Epoch [1720], val_loss: 0.4716\n",
            "Epoch [1740], val_loss: 0.4707\n",
            "Epoch [1760], val_loss: 0.4699\n",
            "Epoch [1780], val_loss: 0.4691\n",
            "Epoch [1800], val_loss: 0.4683\n",
            "Epoch [1820], val_loss: 0.4675\n",
            "Epoch [1840], val_loss: 0.4669\n",
            "Epoch [1860], val_loss: 0.4663\n",
            "Epoch [1880], val_loss: 0.4658\n",
            "Epoch [1900], val_loss: 0.4654\n",
            "Epoch [1920], val_loss: 0.4649\n",
            "Epoch [1940], val_loss: 0.4645\n",
            "Epoch [1960], val_loss: 0.4642\n",
            "Epoch [1980], val_loss: 0.4639\n",
            "Epoch [2000], val_loss: 0.4636\n",
            "Epoch [2020], val_loss: 0.4634\n",
            "Epoch [2040], val_loss: 0.4631\n",
            "Epoch [2060], val_loss: 0.4629\n",
            "Epoch [2080], val_loss: 0.4626\n",
            "Epoch [2100], val_loss: 0.4624\n",
            "Epoch [2120], val_loss: 0.4623\n",
            "Epoch [2140], val_loss: 0.4621\n",
            "Epoch [2160], val_loss: 0.4620\n",
            "Epoch [2180], val_loss: 0.4619\n",
            "Epoch [2200], val_loss: 0.4618\n",
            "Epoch [2220], val_loss: 0.4617\n",
            "Epoch [2240], val_loss: 0.4616\n",
            "Epoch [2260], val_loss: 0.4615\n",
            "Epoch [2280], val_loss: 0.4614\n",
            "Epoch [2300], val_loss: 0.4613\n",
            "Epoch [2320], val_loss: 0.4613\n",
            "Epoch [2340], val_loss: 0.4612\n",
            "Epoch [2360], val_loss: 0.4611\n",
            "Epoch [2380], val_loss: 0.4610\n",
            "Epoch [2400], val_loss: 0.4610\n",
            "Epoch [2420], val_loss: 0.4609\n",
            "Epoch [2440], val_loss: 0.4608\n",
            "Epoch [2460], val_loss: 0.4608\n",
            "Epoch [2480], val_loss: 0.4607\n",
            "Epoch [2500], val_loss: 0.4607\n",
            "Epoch [2520], val_loss: 0.4606\n",
            "Epoch [2540], val_loss: 0.4606\n",
            "Epoch [2560], val_loss: 0.4606\n",
            "Epoch [2580], val_loss: 0.4605\n",
            "Epoch [2600], val_loss: 0.4605\n",
            "Epoch [2620], val_loss: 0.4605\n",
            "Epoch [2640], val_loss: 0.4604\n",
            "Epoch [2660], val_loss: 0.4604\n",
            "Epoch [2680], val_loss: 0.4604\n",
            "Epoch [2700], val_loss: 0.4604\n",
            "Epoch [2720], val_loss: 0.4603\n",
            "Epoch [2740], val_loss: 0.4603\n",
            "Epoch [2760], val_loss: 0.4603\n",
            "Epoch [2780], val_loss: 0.4603\n",
            "Epoch [2800], val_loss: 0.4602\n",
            "Epoch [2820], val_loss: 0.4602\n",
            "Epoch [2840], val_loss: 0.4602\n",
            "Epoch [2860], val_loss: 0.4602\n",
            "Epoch [2880], val_loss: 0.4601\n",
            "Epoch [2900], val_loss: 0.4601\n",
            "Epoch [2920], val_loss: 0.4601\n",
            "Epoch [2940], val_loss: 0.4601\n",
            "Epoch [2960], val_loss: 0.4601\n",
            "Epoch [2980], val_loss: 0.4600\n",
            "Epoch [3000], val_loss: 0.4600\n",
            "Epoch [3020], val_loss: 0.4600\n",
            "Epoch [3040], val_loss: 0.4600\n",
            "Epoch [3060], val_loss: 0.4600\n",
            "Epoch [3080], val_loss: 0.4600\n",
            "Epoch [3100], val_loss: 0.4599\n",
            "Epoch [3120], val_loss: 0.4599\n",
            "Epoch [3140], val_loss: 0.4599\n",
            "Epoch [3160], val_loss: 0.4599\n",
            "Epoch [3180], val_loss: 0.4599\n",
            "Epoch [3200], val_loss: 0.4599\n",
            "Epoch [3220], val_loss: 0.4599\n",
            "Epoch [3240], val_loss: 0.4598\n",
            "Epoch [3260], val_loss: 0.4598\n",
            "Epoch [3280], val_loss: 0.4598\n",
            "Epoch [3300], val_loss: 0.4598\n",
            "Epoch [3320], val_loss: 0.4598\n",
            "Epoch [3340], val_loss: 0.4598\n",
            "Epoch [3360], val_loss: 0.4598\n",
            "Epoch [3380], val_loss: 0.4597\n",
            "Epoch [3400], val_loss: 0.4597\n",
            "Epoch [3420], val_loss: 0.4597\n",
            "Epoch [3440], val_loss: 0.4597\n",
            "Epoch [3460], val_loss: 0.4597\n",
            "Epoch [3480], val_loss: 0.4597\n",
            "Epoch [3500], val_loss: 0.4597\n",
            "Epoch [3520], val_loss: 0.4597\n",
            "Epoch [3540], val_loss: 0.4596\n",
            "Epoch [3560], val_loss: 0.4596\n",
            "Epoch [3580], val_loss: 0.4596\n",
            "Epoch [3600], val_loss: 0.4596\n",
            "Epoch [3620], val_loss: 0.4596\n",
            "Epoch [3640], val_loss: 0.4596\n",
            "Epoch [3660], val_loss: 0.4596\n",
            "Epoch [3680], val_loss: 0.4596\n",
            "Epoch [3700], val_loss: 0.4595\n",
            "Epoch [3720], val_loss: 0.4595\n",
            "Epoch [3740], val_loss: 0.4595\n",
            "Epoch [3760], val_loss: 0.4595\n",
            "Epoch [3780], val_loss: 0.4595\n",
            "Epoch [3800], val_loss: 0.4595\n",
            "Epoch [3820], val_loss: 0.4595\n",
            "Epoch [3840], val_loss: 0.4595\n",
            "Epoch [3860], val_loss: 0.4594\n",
            "Epoch [3880], val_loss: 0.4594\n",
            "Epoch [3900], val_loss: 0.4594\n",
            "Epoch [3920], val_loss: 0.4594\n",
            "Epoch [3940], val_loss: 0.4594\n",
            "Epoch [3960], val_loss: 0.4594\n",
            "Epoch [3980], val_loss: 0.4594\n",
            "Epoch [4000], val_loss: 0.4594\n",
            "Epoch [4020], val_loss: 0.4593\n",
            "Epoch [4040], val_loss: 0.4593\n",
            "Epoch [4060], val_loss: 0.4593\n",
            "Epoch [4080], val_loss: 0.4593\n",
            "Epoch [4100], val_loss: 0.4593\n",
            "Epoch [4120], val_loss: 0.4593\n",
            "Epoch [4140], val_loss: 0.4593\n",
            "Epoch [4160], val_loss: 0.4593\n",
            "Epoch [4180], val_loss: 0.4593\n",
            "Epoch [4200], val_loss: 0.4592\n",
            "Epoch [4220], val_loss: 0.4592\n",
            "Epoch [4240], val_loss: 0.4592\n",
            "Epoch [4260], val_loss: 0.4592\n",
            "Epoch [4280], val_loss: 0.4592\n",
            "Epoch [4300], val_loss: 0.4592\n",
            "Epoch [4320], val_loss: 0.4592\n",
            "Epoch [4340], val_loss: 0.4592\n",
            "Epoch [4360], val_loss: 0.4591\n",
            "Epoch [4380], val_loss: 0.4591\n",
            "Epoch [4400], val_loss: 0.4591\n",
            "Epoch [4420], val_loss: 0.4591\n",
            "Epoch [4440], val_loss: 0.4591\n",
            "Epoch [4460], val_loss: 0.4591\n",
            "Epoch [4480], val_loss: 0.4591\n",
            "Epoch [4500], val_loss: 0.4591\n",
            "Epoch [4520], val_loss: 0.4590\n",
            "Epoch [4540], val_loss: 0.4590\n",
            "Epoch [4560], val_loss: 0.4590\n",
            "Epoch [4580], val_loss: 0.4590\n",
            "Epoch [4600], val_loss: 0.4590\n",
            "Epoch [4620], val_loss: 0.4590\n",
            "Epoch [4640], val_loss: 0.4590\n",
            "Epoch [4660], val_loss: 0.4590\n",
            "Epoch [4680], val_loss: 0.4589\n",
            "Epoch [4700], val_loss: 0.4589\n",
            "Epoch [4720], val_loss: 0.4589\n",
            "Epoch [4740], val_loss: 0.4589\n",
            "Epoch [4760], val_loss: 0.4589\n",
            "Epoch [4780], val_loss: 0.4589\n",
            "Epoch [4800], val_loss: 0.4589\n",
            "Epoch [4820], val_loss: 0.4589\n",
            "Epoch [4840], val_loss: 0.4589\n",
            "Epoch [4860], val_loss: 0.4588\n",
            "Epoch [4880], val_loss: 0.4588\n",
            "Epoch [4900], val_loss: 0.4588\n",
            "Epoch [4920], val_loss: 0.4588\n",
            "Epoch [4940], val_loss: 0.4588\n",
            "Epoch [4960], val_loss: 0.4588\n",
            "Epoch [4980], val_loss: 0.4588\n",
            "Epoch [5000], val_loss: 0.4588\n"
          ]
        }
      ],
      "source": [
        "epochs = 5000\n",
        "lr = 1e-7\n",
        "history1 = fit(epochs, lr, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "8baf1ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_single(input, target, model):\n",
        "    inputs = input.unsqueeze(0)\n",
        "    predictions = model(inputs)                \n",
        "    prediction = predictions[0].detach()\n",
        "    return target, prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "eb490bd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "targets = []\n",
        "predictions = []\n",
        "for i in range(len(val_ds)):\n",
        "    input, target = val_ds[i]\n",
        "    target, prediction = predict_single(input, target, model)\n",
        "    targets.append(target)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "xtargets = []\n",
        "ytargets = []\n",
        "xpredictions = []\n",
        "ypredictions = []\n",
        "for i in range(len(targets)):\n",
        "    xtargets.append(targets[i][0])\n",
        "    ytargets.append(targets[i][1])\n",
        "    xpredictions.append(predictions[i][0])\n",
        "    ypredictions.append(predictions[i][1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "fb6700ff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x1abe35f3b50>"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "ax.scatter(xtargets, ytargets)\n",
        "ax.scatter(xpredictions, ypredictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad3565f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
